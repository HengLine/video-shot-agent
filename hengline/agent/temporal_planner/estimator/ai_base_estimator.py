"""
@FileName: ai_estimator.py
@Description: AIä¼°ç®—å™¨åŸºç±»
@Author: HengLine
@Time: 2026/1/12 21:03
"""
import hashlib
import json
import re
import time
from abc import ABC, abstractmethod
from datetime import datetime
from typing import List, Dict, Any

from hengline.agent.temporal_planner.base_temporal_planner import EstimationError, EstimationErrorLevel
from hengline.agent.temporal_planner.temporal_planner_model import DurationEstimation, ElementType
from hengline.prompts.temporal_planner_prompt import PromptConfig
from hengline.prompts.temporal_planner_specialized_prompt import SpecializedPromptTemplates


class BaseAIDurationEstimator(ABC):
    """AIä¼°ç®—å™¨åŸºç±»"""
    def __init__(self, llm_client, config: PromptConfig = None):
        self.llm = llm_client
        self.config = config or PromptConfig()
        self.prompt_templates = SpecializedPromptTemplates(config)

        self.error_log: List[EstimationError] = []
        self.cache: Dict[str, DurationEstimation] = {}

        self.error_log: List[EstimationError] = []
        self.cache: Dict[str, DurationEstimation] = {}

    # ============================ æŠ½è±¡å±æ€§ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰============================
    @abstractmethod
    def _get_element_type(self) -> ElementType:
        """è·å–å…ƒç´ ç±»å‹"""
        pass

    @abstractmethod
    def _get_id_value(self, element_data: Any) -> str:
        """è·å–å…ƒç´ IDå­—æ®µå"""
        pass

    @abstractmethod
    def _parse_ai_response(self, response: str, element_data: Any) -> Dict[str, Any] | None:
        """è§£æAIå“åº”ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _validate_estimation(self, parsed_result: Dict, element_data: Any) -> DurationEstimation:
        """éªŒè¯ä¼°ç®—ç»“æœï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _create_fallback_estimation(self, element_data: Any, context: Dict = None) -> DurationEstimation:
        """åˆ›å»ºé™çº§ä¼°ç®—ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    @abstractmethod
    def _generate_prompt(self, element_data: Any, context: Dict = None) -> str:
        """ç”Ÿæˆæç¤ºè¯ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        # åŸºç¡€æç¤ºè¯ç”±å­ç±»å®ç°
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _generate_prompt æ–¹æ³•")


    # ============================ å…¬å…±æ–¹æ³• ============================
    def estimate(self, element_data: Any, context: Dict = None) -> DurationEstimation:
        """å…¬å…±æ¥å£ï¼šä¼°ç®—å…ƒç´ æ—¶é•¿"""
        return self.estimate_with_context(element_data, context)


    def estimate_with_context(self, element_data: Any, context: Dict = None) -> DurationEstimation:
        """å¸¦ä¸Šä¸‹æ–‡çš„ä¼°ç®—"""
        start_time = datetime.now()

        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(element_data, context)
            if cache_key in self.cache:
                cached = self.cache[cache_key]
                cached.timestamp = datetime.now().isoformat()  # æ›´æ–°æ—¶é—´æˆ³
                return cached

            # ç”Ÿæˆæç¤ºè¯
            prompt = self._generate_prompt(element_data, context)
            prompt_hash = self._hash_prompt(prompt)

            # è°ƒç”¨AI
            raw_response = self._call_llm_with_retry(prompt)

            # è§£æå“åº”
            parsed_result = self._parse_ai_response(raw_response, element_data)

            # éªŒè¯å’Œå¢å¼º
            result = self._validate_estimation(parsed_result, element_data)
            result = self._enhance_estimation(result, element_data)

            # æ·»åŠ å…ƒæ•°æ®
            result.prompt_hash = prompt_hash
            result.timestamp = datetime.now().isoformat()
            result.processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            result.original_data = element_data.copy()
            result.raw_ai_response = raw_response

            # ç¼“å­˜ç»“æœ
            self.cache[cache_key] = result

            return result

        except Exception as e:
            return self._handle_estimation_error(element_data, context, str(e), start_time)

    def batch_estimate(self, elements: List[Dict], context: Dict = None) -> List[DurationEstimation]:
        """æ‰¹é‡ä¼°ç®—"""
        results = []

        for element_data in elements:
            result = self.estimate_with_context(element_data, context)
            results.append(result)

            # æ›´æ–°ä¸Šä¸‹æ–‡ä¾›ä¸‹ä¸€ä¸ªå…ƒç´ ä½¿ç”¨
            if context is not None:
                context = self._update_context(context, result)

        return results

    def _call_llm_with_retry(self, prompt: str, max_retries: int = 3) -> Any | None:
        """è°ƒç”¨LLMï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(max_retries):
            try:
                return self.llm.invoke(prompt)

            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                time.sleep(1)

    def _enhance_estimation(self, result: DurationEstimation, element_data: Any) -> DurationEstimation:
        """å¢å¼ºä¼°ç®—ç»“æœï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        # åŸºç¡€å¢å¼ºï¼šæ·»åŠ æ—¶é—´æˆ³
        if not result.timestamp:
            result.timestamp = datetime.now().isoformat()
        return result

    def _update_context(self, context: Dict, result: DurationEstimation) -> Dict:
        """æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        if "processed_elements" not in context:
            context["processed_elements"] = []

        context["processed_elements"].append({
            "element_id": result.element_id,
            "element_type": result.element_type.value,
            "duration": result.estimated_duration,
            "confidence": result.confidence
        })

        return context

    def _generate_cache_key(self, element_data: Any, context: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        element_id = self._get_id_value(element_data)
        element_type = self._get_element_type().value

        # ä½¿ç”¨å…ƒç´ å†…å®¹å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå“ˆå¸Œ
        content_str = json.dumps(element_data, sort_keys=True)
        context_str = json.dumps(context, sort_keys=True) if context else ""

        combined = f"{element_type}:{element_id}:{content_str}:{context_str}"
        return hashlib.md5(combined.encode('utf-8')).hexdigest()

    def _hash_prompt(self, prompt: str) -> str:
        """è®¡ç®—æç¤ºè¯å“ˆå¸Œ"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()[:8]

    def _clean_json_response(self, response: str) -> str:
        """æ¸…ç†JSONå“åº”"""
        # ç§»é™¤Markdownä»£ç å—
        cleaned = re.sub(r'```json\s*', '', response)
        cleaned = re.sub(r'\s*```', '', cleaned)
        cleaned = cleaned.strip()

        # æå–JSONå¯¹è±¡
        start = cleaned.find('{')
        end = cleaned.rfind('}') + 1

        if start >= 0 and end > start:
            cleaned = cleaned[start:end]

        return cleaned

    def _log_error(self, element_id: str, error_type: str, message: str,
                   level: EstimationErrorLevel, recovery_action: str = "",
                   fallback_value: float = None):
        """è®°å½•é”™è¯¯"""
        error = EstimationError(
            element_id=element_id,
            error_type=error_type,
            message=message,
            level=level,
            recovery_action=recovery_action,
            fallback_value=fallback_value,
            timestamp=datetime.now().isoformat()
        )

        self.error_log.append(error)

        # æ‰“å°é”™è¯¯ä¿¡æ¯
        level_icon = {
            EstimationErrorLevel.WARNING: "âš ï¸",
            EstimationErrorLevel.ERROR: "âŒ",
            EstimationErrorLevel.CRITICAL: "ğŸ”¥"
        }.get(level, "â„¹ï¸")

        print(f"{level_icon} [{level.value.upper()}] {error_type}: {message}")
        if recovery_action:
            print(f"  æ¢å¤æ“ä½œ: {recovery_action}")

    def _handle_estimation_error(self, element_data: Any, context: Dict,
                                 error_message: str, start_time: datetime) -> DurationEstimation:
        """å¤„ç†ä¼°ç®—é”™è¯¯"""
        self._log_error(
            element_id=self._get_id_value(element_data),
            error_type="estimation_failed",
            message=f"{self._get_element_type().value}ä¼°ç®—å¤±è´¥: {error_message}",
            level=EstimationErrorLevel.ERROR,
            recovery_action="ä½¿ç”¨é™çº§ä¼°ç®—",
            fallback_value=0.0
        )

        # åˆ›å»ºé™çº§ä¼°ç®—
        fallback_result = self._create_fallback_estimation(element_data, context)

        # æ·»åŠ é”™è¯¯å…ƒæ•°æ®
        fallback_result.timestamp = datetime.now().isoformat()
        fallback_result.processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        fallback_result.confidence = min(fallback_result.confidence, 0.4)  # é™ä½ç½®ä¿¡åº¦

        return fallback_result

    def get_error_summary(self) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        error_counts = {}
        for error in self.error_log:
            error_counts[error.error_type] = error_counts.get(error.error_type, 0) + 1

        return {
            "total_errors": len(self.error_log),
            "error_by_type": error_counts,
            "errors_by_level": {
                "warning": len([e for e in self.error_log if e.level == EstimationErrorLevel.WARNING]),
                "error": len([e for e in self.error_log if e.level == EstimationErrorLevel.ERROR]),
                "critical": len([e for e in self.error_log if e.level == EstimationErrorLevel.CRITICAL])
            }
        }

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()

    def clear_errors(self):
        """æ¸…ç©ºé”™è¯¯æ—¥å¿—"""
        self.error_log.clear()
